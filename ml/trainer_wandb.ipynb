{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fbe8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimian\u001b[0m (\u001b[33mtimian-vegg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Suppress the specific FutureWarning from Hugging Face Transformers\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`\",\n",
    "    category=FutureWarning,\n",
    "    module=\"transformers.trainer\" # More specific to avoid suppressing other warnings\n",
    ")\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d536352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3\n",
    "output_dir = f\"sentiment_model-{time.strftime(\"%Y%m%d-%H%M%S\")}\"\n",
    "#os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',  # random, grid, or bayes\n",
    "    'metric': {\n",
    "        'name': 'eval_f1',  # Or 'eval_accuracy', 'eval_loss'\n",
    "        'goal': 'maximize'  # maximize or minimize\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',  # Use log scale for finer search\n",
    "            'min': 1.0e-6,\n",
    "            'max': 5.0e-5\n",
    "        },\n",
    "        'per_device_train_batch_size': {\n",
    "            'values': [8, 16]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [8, 10]\n",
    "        },\n",
    "        'early_stopping_patience': {\n",
    "            'values': [3, 5]\n",
    "        },\n",
    "        # New parameters to help prevent overfitting:\n",
    "        'dropout_rate': {\n",
    "            'values': [0.1, 0.2, 0.3]  # Adjust based on your model’s support for dropout\n",
    "        },\n",
    "        'label_smoothing': {\n",
    "            'values': [0.0, 0.1, 0.2]  # 0.0 means no smoothing\n",
    "        },\n",
    "        # Optionally, if your training code supports it:\n",
    "        'max_grad_norm': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.5,\n",
    "            'max': 2.0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ffa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    2535\n",
      "2    1168\n",
      "0     514\n",
      "Name: count, dtype: int64\n",
      "Train size: 2951, Validation size: 633, Test size: 633\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"financial_phrasebank.csv\", usecols=[\"norwegian_sentence\", \"label\"]).rename(columns={\"norwegian_sentence\": \"sentence\"})\n",
    "\n",
    "# Check label distribution\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Split into train (70%), temp (30%) -> then split temp into validation/test (50% each)\n",
    "train_ds, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_ds, test_ds = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}, Validation size: {len(val_ds)}, Test size: {len(test_ds)}\")\n",
    "\n",
    "# Save datasets\n",
    "train_ds.to_csv(\"train.csv\", index=False)\n",
    "val_ds.to_csv(\"validation.csv\", index=False)\n",
    "test_ds.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725d8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2951 examples [00:00, 86647.89 examples/s]\n",
      "Generating validation split: 633 examples [00:00, 94272.43 examples/s]\n",
      "Generating test split: 633 examples [00:00, 79405.26 examples/s]\n",
      "Map: 100%|██████████| 2951/2951 [00:00<00:00, 9039.69 examples/s]\n",
      "Map: 100%|██████████| 633/633 [00:00<00:00, 7302.89 examples/s]\n",
      "Map: 100%|██████████| 633/633 [00:00<00:00, 6184.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"train.csv\",\n",
    "        \"validation\": \"validation.csv\",\n",
    "        \"test\": \"test.csv\",\n",
    "    },\n",
    ")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545efb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    wandb.init(project=\"nb-bert-sweep\")\n",
    "    config = wandb.config\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-base\", num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_wandb_sweep/{wandb.run.name}\",\n",
    "        report_to=\"wandb\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_train_batch_size,\n",
    "        greater_is_better=(sweep_config['metric']['goal'] == 'maximize'),    # lower is better for loss\n",
    "        metric_for_best_model=sweep_config['metric']['name'],\n",
    "        weight_decay=config.weight_decay,                                    # To prevent overfitting, TODO NEEDS TUNING, initially increase by a small amount\n",
    "        num_train_epochs=config.num_train_epochs,                            # Use value from wandb.config\n",
    "        learning_rate=config.learning_rate,                                  # Very common starting point for BERT fine-tuning, TODO: try 1e-5, 2e-5, 3e-5, 5e-5, or a linear/cosine scheduler\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config.early_stopping_patience,\n",
    "        early_stopping_threshold=0.001 # A small threshold for improvement\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6325ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1ehfsdt4\n",
      "Sweep URL: https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 62xsc0tv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.0916934518335293e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08448659789034704\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_182032-62xsc0tv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/62xsc0tv' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/62xsc0tv' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/62xsc0tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/3690 15:55 < 03:59, 3.09 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.521550</td>\n",
       "      <td>0.857820</td>\n",
       "      <td>0.859314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.436199</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.576024</td>\n",
       "      <td>0.868878</td>\n",
       "      <td>0.870877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.642492</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.883211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.538513</td>\n",
       "      <td>0.903633</td>\n",
       "      <td>0.903680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.903633</td>\n",
       "      <td>0.904139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.624302</td>\n",
       "      <td>0.903633</td>\n",
       "      <td>0.903707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.621679</td>\n",
       "      <td>0.902054</td>\n",
       "      <td>0.902011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇▃▅████</td></tr><tr><td>eval/f1</td><td>▁▇▃▅████</td></tr><tr><td>eval/loss</td><td>▄▁▆█▄▆▇▇</td></tr><tr><td>eval/runtime</td><td>▁▂▃▂▂▂▂█</td></tr><tr><td>eval/samples_per_second</td><td>█▇▆▆▇▇▇▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▆▆▇▇▇▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▂▂▂▃▂▂▁▁▁▁▁▁▁▁▃▁▁▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▄▃▃▂▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.90205</td></tr><tr><td>eval/f1</td><td>0.90201</td></tr><tr><td>eval/loss</td><td>0.62168</td></tr><tr><td>eval/runtime</td><td>7.5012</td></tr><tr><td>eval/samples_per_second</td><td>84.386</td></tr><tr><td>eval/steps_per_second</td><td>10.665</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>0.01213</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0068</td></tr><tr><td>train_loss</td><td>0.1523</td></tr><tr><td>train_runtime</td><td>956.1451</td></tr><tr><td>train_samples_per_second</td><td>30.864</td></tr><tr><td>train_steps_per_second</td><td>3.859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-sweep-1</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/62xsc0tv' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/62xsc0tv</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_182032-62xsc0tv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9npee25i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.6232528048325252e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08213421038744316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_183638-9npee25i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/9npee25i' target=\"_blank\">ruby-sweep-2</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/9npee25i' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/9npee25i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 13:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.427699</td>\n",
       "      <td>0.853081</td>\n",
       "      <td>0.855913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.343028</td>\n",
       "      <td>0.887836</td>\n",
       "      <td>0.887852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.548713</td>\n",
       "      <td>0.873618</td>\n",
       "      <td>0.873841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.603042</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.590558</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.896647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.598670</td>\n",
       "      <td>0.892575</td>\n",
       "      <td>0.892392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.894294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.629889</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▄▅█▇██</td></tr><tr><td>eval/f1</td><td>▁▆▄▆█▇██</td></tr><tr><td>eval/loss</td><td>▃▁▆▇▇▇▇█</td></tr><tr><td>eval/runtime</td><td>▇▁▅▇▄█▂▆</td></tr><tr><td>eval/samples_per_second</td><td>▂█▄▂▅▁▇▃</td></tr><tr><td>eval/steps_per_second</td><td>▂█▄▂▅▁▇▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▃▂▇▁▁▁▁▁█▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89573</td></tr><tr><td>eval/f1</td><td>0.89591</td></tr><tr><td>eval/loss</td><td>0.62989</td></tr><tr><td>eval/runtime</td><td>7.3234</td></tr><tr><td>eval/samples_per_second</td><td>86.435</td></tr><tr><td>eval/steps_per_second</td><td>5.462</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>1480</td></tr><tr><td>train/grad_norm</td><td>0.0158</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0014</td></tr><tr><td>train_loss</td><td>0.13159</td></tr><tr><td>train_runtime</td><td>832.0452</td></tr><tr><td>train_samples_per_second</td><td>28.373</td></tr><tr><td>train_steps_per_second</td><td>1.779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-2</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/9npee25i' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/9npee25i</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_183638-9npee25i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wttpqr79 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.6366826108318e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07126403723840813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_185046-wttpqr79</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/wttpqr79' target=\"_blank\">toasty-sweep-3</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/wttpqr79' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/wttpqr79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1850' max='1850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1850/1850 15:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>0.391940</td>\n",
       "      <td>0.849921</td>\n",
       "      <td>0.850695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.314116</td>\n",
       "      <td>0.884676</td>\n",
       "      <td>0.884468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.399890</td>\n",
       "      <td>0.884676</td>\n",
       "      <td>0.885172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.436902</td>\n",
       "      <td>0.892575</td>\n",
       "      <td>0.893146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.498458</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.894053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.525481</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.897595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.556893</td>\n",
       "      <td>0.903633</td>\n",
       "      <td>0.903665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.575339</td>\n",
       "      <td>0.905213</td>\n",
       "      <td>0.905135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.575378</td>\n",
       "      <td>0.903633</td>\n",
       "      <td>0.903494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.583181</td>\n",
       "      <td>0.898894</td>\n",
       "      <td>0.899000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▅▆▇▇███▇</td></tr><tr><td>eval/f1</td><td>▁▅▅▆▇▇███▇</td></tr><tr><td>eval/loss</td><td>▃▁▃▄▆▆▇███</td></tr><tr><td>eval/runtime</td><td>▇█▂▁▂▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▇█▇█████</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▇█▇█████</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▁▂▂▅▁▁▅▁▁▁█▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89889</td></tr><tr><td>eval/f1</td><td>0.899</td></tr><tr><td>eval/loss</td><td>0.58318</td></tr><tr><td>eval/runtime</td><td>6.469</td></tr><tr><td>eval/samples_per_second</td><td>97.852</td></tr><tr><td>eval/steps_per_second</td><td>6.183</td></tr><tr><td>total_flos</td><td>1941119239288320.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>1850</td></tr><tr><td>train/grad_norm</td><td>0.02714</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0105</td></tr><tr><td>train_loss</td><td>0.13632</td></tr><tr><td>train_runtime</td><td>949.695</td></tr><tr><td>train_samples_per_second</td><td>31.073</td></tr><tr><td>train_steps_per_second</td><td>1.948</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-3</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/wttpqr79' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/wttpqr79</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_185046-wttpqr79\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k77xgq7b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.860587439264362e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.08002614140950227\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_190644-k77xgq7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/k77xgq7b' target=\"_blank\">rose-sweep-4</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/k77xgq7b' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/k77xgq7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='2952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/2952 15:24, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.394981</td>\n",
       "      <td>0.857820</td>\n",
       "      <td>0.859192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.367495</td>\n",
       "      <td>0.884676</td>\n",
       "      <td>0.884508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.485570</td>\n",
       "      <td>0.889415</td>\n",
       "      <td>0.890558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.593808</td>\n",
       "      <td>0.878357</td>\n",
       "      <td>0.879630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.613720</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.894475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.658930</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▇▅████</td></tr><tr><td>eval/f1</td><td>▁▆▇▅████</td></tr><tr><td>eval/loss</td><td>▂▁▄▆▆▇██</td></tr><tr><td>eval/runtime</td><td>▁▁▂█▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>██▇▁▇▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▁▇▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▃▃▃▂▄▁█▁▁▁▁▁▁▁▄▁▁▁▁▅▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▄▃▃▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89573</td></tr><tr><td>eval/f1</td><td>0.89594</td></tr><tr><td>eval/loss</td><td>0.65893</td></tr><tr><td>eval/runtime</td><td>6.6636</td></tr><tr><td>eval/samples_per_second</td><td>94.993</td></tr><tr><td>eval/steps_per_second</td><td>12.005</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>0.04712</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0121</td></tr><tr><td>train_loss</td><td>0.15346</td></tr><tr><td>train_runtime</td><td>924.6005</td></tr><tr><td>train_samples_per_second</td><td>25.533</td></tr><tr><td>train_steps_per_second</td><td>3.193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-sweep-4</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/k77xgq7b' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/k77xgq7b</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_190644-k77xgq7b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c70t5fuu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.310079626971827e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.09199042654297128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_192215-c70t5fuu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/c70t5fuu' target=\"_blank\">dulcet-sweep-5</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/c70t5fuu' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/c70t5fuu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='2952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/2952 15:24, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.698272</td>\n",
       "      <td>0.807267</td>\n",
       "      <td>0.810990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.472978</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.870617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.586409</td>\n",
       "      <td>0.864139</td>\n",
       "      <td>0.865524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.610973</td>\n",
       "      <td>0.857820</td>\n",
       "      <td>0.859665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.868878</td>\n",
       "      <td>0.867752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.795861</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.871689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.752478</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.882952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.786216</td>\n",
       "      <td>0.884676</td>\n",
       "      <td>0.884640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇▆▆▇▇██</td></tr><tr><td>eval/f1</td><td>▁▇▆▆▆▇██</td></tr><tr><td>eval/loss</td><td>▆▁▃▄▇█▇█</td></tr><tr><td>eval/runtime</td><td>▂▂▃▁▁█▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▆██▁▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▆██▁█▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88468</td></tr><tr><td>eval/f1</td><td>0.88464</td></tr><tr><td>eval/loss</td><td>0.78622</td></tr><tr><td>eval/runtime</td><td>6.6665</td></tr><tr><td>eval/samples_per_second</td><td>94.953</td></tr><tr><td>eval/steps_per_second</td><td>12</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>319.85199</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0076</td></tr><tr><td>train_loss</td><td>0.21337</td></tr><tr><td>train_runtime</td><td>924.8853</td></tr><tr><td>train_samples_per_second</td><td>25.525</td></tr><tr><td>train_steps_per_second</td><td>3.192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-5</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/c70t5fuu' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/c70t5fuu</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_192215-c70t5fuu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cl3nm98o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.0850358140210476e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.007259116193192539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_193746-cl3nm98o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/cl3nm98o' target=\"_blank\">amber-sweep-6</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/cl3nm98o' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/cl3nm98o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='2952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/2952 22:19, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.457612</td>\n",
       "      <td>0.865719</td>\n",
       "      <td>0.867442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.424336</td>\n",
       "      <td>0.889415</td>\n",
       "      <td>0.889881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.500384</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>0.892437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.559887</td>\n",
       "      <td>0.889415</td>\n",
       "      <td>0.889938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.635105</td>\n",
       "      <td>0.902054</td>\n",
       "      <td>0.901737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.681528</td>\n",
       "      <td>0.900474</td>\n",
       "      <td>0.900520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.710363</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.897265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.707507</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▆▆██▇▇</td></tr><tr><td>eval/f1</td><td>▁▆▆▆██▇▇</td></tr><tr><td>eval/loss</td><td>▂▁▃▄▆▇██</td></tr><tr><td>eval/runtime</td><td>▁▁▁▇▂███</td></tr><tr><td>eval/samples_per_second</td><td>██▇▁▇▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>██▇▁▇▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▂▃▅▁▂▂█▂▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89573</td></tr><tr><td>eval/f1</td><td>0.89573</td></tr><tr><td>eval/loss</td><td>0.70751</td></tr><tr><td>eval/runtime</td><td>14.2942</td></tr><tr><td>eval/samples_per_second</td><td>44.284</td></tr><tr><td>eval/steps_per_second</td><td>5.597</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>0.00909</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0143</td></tr><tr><td>train_loss</td><td>0.14833</td></tr><tr><td>train_runtime</td><td>1339.6941</td></tr><tr><td>train_samples_per_second</td><td>17.622</td></tr><tr><td>train_steps_per_second</td><td>2.203</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-6</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/cl3nm98o' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/cl3nm98o</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_193746-cl3nm98o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ndcpo96r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.7371715657881995e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01523512894421042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_200015-ndcpo96r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ndcpo96r' target=\"_blank\">winter-sweep-7</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ndcpo96r' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ndcpo96r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='2952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/2952 20:58, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.600386</td>\n",
       "      <td>0.848341</td>\n",
       "      <td>0.850909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>0.886256</td>\n",
       "      <td>0.885642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.563761</td>\n",
       "      <td>0.864139</td>\n",
       "      <td>0.866721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.884676</td>\n",
       "      <td>0.884120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.656590</td>\n",
       "      <td>0.887836</td>\n",
       "      <td>0.887374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.696839</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>0.881880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.715859</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>0.891555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.740779</td>\n",
       "      <td>0.887836</td>\n",
       "      <td>0.888135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇▄▇▇▆█▇</td></tr><tr><td>eval/f1</td><td>▁▇▄▇▇▆█▇</td></tr><tr><td>eval/loss</td><td>▅▁▄▅▆▇▇█</td></tr><tr><td>eval/runtime</td><td>██▁▁▁▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▇██▇▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▇██▇▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▂▃▁█▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88784</td></tr><tr><td>eval/f1</td><td>0.88814</td></tr><tr><td>eval/loss</td><td>0.74078</td></tr><tr><td>eval/runtime</td><td>7.8345</td></tr><tr><td>eval/samples_per_second</td><td>80.797</td></tr><tr><td>eval/steps_per_second</td><td>10.211</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>0.00644</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0009</td></tr><tr><td>train_loss</td><td>0.15683</td></tr><tr><td>train_runtime</td><td>1258.899</td></tr><tr><td>train_samples_per_second</td><td>18.753</td></tr><tr><td>train_steps_per_second</td><td>2.345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-7</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ndcpo96r' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ndcpo96r</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_200015-ndcpo96r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y2nqqxwl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.826953415783819e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.06047921939953729\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_202121-y2nqqxwl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/y2nqqxwl' target=\"_blank\">playful-sweep-8</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/y2nqqxwl' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/y2nqqxwl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2952' max='2952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2952/2952 17:37, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.598105</td>\n",
       "      <td>0.808847</td>\n",
       "      <td>0.811613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.457320</td>\n",
       "      <td>0.870458</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.362068</td>\n",
       "      <td>0.887836</td>\n",
       "      <td>0.887405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.517722</td>\n",
       "      <td>0.887836</td>\n",
       "      <td>0.886567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.555008</td>\n",
       "      <td>0.889415</td>\n",
       "      <td>0.888467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.893649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.579661</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.897373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.584776</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.896121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▇▇▇███</td></tr><tr><td>eval/f1</td><td>▁▆▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▄▁▆▇▇▇█</td></tr><tr><td>eval/runtime</td><td>▇█▇███▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▂▁▁▁██</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▂▁▁▁██</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▃▁▃▂▂▁█▁▃▁▁▁▇▁▁▂▃▁▁▁▃▁▁▁▁▁▁▄</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▅▅▅▄▄▃▄▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89573</td></tr><tr><td>eval/f1</td><td>0.89612</td></tr><tr><td>eval/loss</td><td>0.58478</td></tr><tr><td>eval/runtime</td><td>6.7947</td></tr><tr><td>eval/samples_per_second</td><td>93.16</td></tr><tr><td>eval/steps_per_second</td><td>11.774</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>2952</td></tr><tr><td>train/grad_norm</td><td>28.54753</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0579</td></tr><tr><td>train_loss</td><td>0.23593</td></tr><tr><td>train_runtime</td><td>1057.1959</td></tr><tr><td>train_samples_per_second</td><td>22.331</td></tr><tr><td>train_steps_per_second</td><td>2.792</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-8</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/y2nqqxwl' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/y2nqqxwl</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_202121-y2nqqxwl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ze4xeihw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.581289876788132e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.002658876217984474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_203906-ze4xeihw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ze4xeihw' target=\"_blank\">vocal-sweep-9</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ze4xeihw' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ze4xeihw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3321' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3321/3690 17:39 < 01:57, 3.13 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.483912</td>\n",
       "      <td>0.840442</td>\n",
       "      <td>0.842506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.397320</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>0.877354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.514461</td>\n",
       "      <td>0.868878</td>\n",
       "      <td>0.870565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.694536</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.873278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.635060</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.883530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.701904</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.893691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.721956</td>\n",
       "      <td>0.892575</td>\n",
       "      <td>0.892772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.803798</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>0.890321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.894155</td>\n",
       "      <td>0.893808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▅▅▇████</td></tr><tr><td>eval/f1</td><td>▁▆▅▅▇████</td></tr><tr><td>eval/loss</td><td>▂▁▃▆▅▆▇█▇</td></tr><tr><td>eval/runtime</td><td>▁▇▂▁▃▅▄█▆</td></tr><tr><td>eval/samples_per_second</td><td>█▂▇█▆▄▅▁▃</td></tr><tr><td>eval/steps_per_second</td><td>█▂▇█▆▄▅▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▂▄▃▁▅▂█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89415</td></tr><tr><td>eval/f1</td><td>0.89381</td></tr><tr><td>eval/loss</td><td>0.7702</td></tr><tr><td>eval/runtime</td><td>6.8043</td></tr><tr><td>eval/samples_per_second</td><td>93.03</td></tr><tr><td>eval/steps_per_second</td><td>11.757</td></tr><tr><td>total_flos</td><td>1747007315359488.0</td></tr><tr><td>train/epoch</td><td>9</td></tr><tr><td>train/global_step</td><td>3321</td></tr><tr><td>train/grad_norm</td><td>0.00298</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0119</td></tr><tr><td>train_loss</td><td>0.15399</td></tr><tr><td>train_runtime</td><td>1059.4956</td></tr><tr><td>train_samples_per_second</td><td>27.853</td></tr><tr><td>train_steps_per_second</td><td>3.483</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-9</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ze4xeihw' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/ze4xeihw</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_203906-ze4xeihw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w2s37izd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.939508600928615e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.029069016856393005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'nb-bert-sweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Development\\Python_programs\\algorithmic-trading\\wandb\\run-20250611_205700-w2s37izd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/w2s37izd' target=\"_blank\">dashing-sweep-10</a></strong> to <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/sweeps/1ehfsdt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/w2s37izd' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/w2s37izd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.position_embeddings.weight, classifier.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.pooler.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.word_embeddings.weight, classifier.weight, bert.embeddings.LayerNorm.weight\n",
      "C:\\Users\\Timia\\AppData\\Local\\Temp\\ipykernel_18980\\696991389.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 12:59, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.561834</td>\n",
       "      <td>0.775671</td>\n",
       "      <td>0.732604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>0.322853</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>0.876549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.399453</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.872828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.458142</td>\n",
       "      <td>0.886256</td>\n",
       "      <td>0.887201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.474937</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>0.891274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.516741</td>\n",
       "      <td>0.892575</td>\n",
       "      <td>0.892743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.540677</td>\n",
       "      <td>0.897314</td>\n",
       "      <td>0.897529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.546254</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.895876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇▇▇████</td></tr><tr><td>eval/f1</td><td>▁▇▇█████</td></tr><tr><td>eval/loss</td><td>█▁▃▅▅▇▇█</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁█▃▆▄</td></tr><tr><td>eval/samples_per_second</td><td>████▁▆▃▅</td></tr><tr><td>eval/steps_per_second</td><td>████▁▆▃▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▄▃▃▁▁█▁▁▅▅▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89573</td></tr><tr><td>eval/f1</td><td>0.89588</td></tr><tr><td>eval/loss</td><td>0.54625</td></tr><tr><td>eval/runtime</td><td>7.0267</td></tr><tr><td>eval/samples_per_second</td><td>90.084</td></tr><tr><td>eval/steps_per_second</td><td>5.693</td></tr><tr><td>total_flos</td><td>1552895391430656.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>1480</td></tr><tr><td>train/grad_norm</td><td>0.04062</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0179</td></tr><tr><td>train_loss</td><td>0.19777</td></tr><tr><td>train_runtime</td><td>780.0367</td></tr><tr><td>train_samples_per_second</td><td>30.265</td></tr><tr><td>train_steps_per_second</td><td>1.897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-10</strong> at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep/runs/w2s37izd' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep/runs/w2s37izd</a><br> View project at: <a href='https://wandb.ai/timian-vegg/nb-bert-sweep' target=\"_blank\">https://wandb.ai/timian-vegg/nb-bert-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250611_205700-w2s37izd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"nb-bert-sweep\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train_model, count=10)\n",
    "#last id: 1ehfsdt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    wandb.init(project=\"nb-bert-sweep\")\n",
    "    config = wandb.config\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-base\", num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_wandb_sweep/{wandb.run.name}\",\n",
    "        report_to=\"wandb\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_train_batch_size,\n",
    "        greater_is_better=(sweep_config['metric']['goal'] == 'maximize'),    # lower is better for loss\n",
    "        metric_for_best_model=sweep_config['metric']['name'],\n",
    "        weight_decay=config.weight_decay,                                    # To prevent overfitting, TODO NEEDS TUNING, initially increase by a small amount\n",
    "        num_train_epochs=config.num_train_epochs,                            # Use value from wandb.config\n",
    "        learning_rate=config.learning_rate,                                  # Very common starting point for BERT fine-tuning, TODO: try 1e-5, 2e-5, 3e-5, 5e-5, or a linear/cosine scheduler\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config.early_stopping_patience,\n",
    "        early_stopping_threshold=0.001 # A small threshold for improvement\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algtrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
